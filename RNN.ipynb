{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"RNN.ipynb","provenance":[],"authorship_tag":"ABX9TyNVGS06irCgBrjWpM3eQfvo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"VrnsL5Hx5FVw","executionInfo":{"status":"ok","timestamp":1617120708213,"user_tz":-420,"elapsed":787,"user":{"displayName":"Phúc Nguyễn","photoUrl":"","userId":"16708820210452606118"}}},"source":["import numpy as np\n","\n","def softmax(x):\n","    xt = np.exp(x - np.max(x))\n","    return xt / np.sum(xt)\n","\n","def save_model_parameters_theano(outfile, model):\n","    U, V, W = model.U.get_value(), model.V.get_value(), model.W.get_value()\n","    np.savez(outfile, U=U, V=V, W=W)\n","    print (\"Saved model parameters to %s.\" % outfile)\n","   \n","def load_model_parameters_theano(path, model):\n","    npzfile = np.load(path)\n","    U, V, W = npzfile[\"U\"], npzfile[\"V\"], npzfile[\"W\"]\n","    model.hidden_dim = U.shape[0]\n","    model.word_dim = U.shape[1]\n","    model.U.set_value(U)\n","    model.V.set_value(V)\n","    model.W.set_value(W)\n","    print(\"Loaded model parameters from %s. hidden_dim=%d word_dim=%d\" % (path, U.shape[0], U.shape[1]))"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"VqKHmLnu6BTM","executionInfo":{"status":"ok","timestamp":1617120757657,"user_tz":-420,"elapsed":4234,"user":{"displayName":"Phúc Nguyễn","photoUrl":"","userId":"16708820210452606118"}}},"source":["import theano as theano\n","import theano.tensor as T\n","import operator\n","\n","class RNNTheano:\n","    \n","    def __init__(self, word_dim, hidden_dim=100, bptt_truncate=4):\n","        # Assign instance variables\n","        self.word_dim = word_dim\n","        self.hidden_dim = hidden_dim\n","        self.bptt_truncate = bptt_truncate\n","        # Randomly initialize the network parameters\n","        U = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), (hidden_dim, word_dim))\n","        V = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (word_dim, hidden_dim))\n","        W = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (hidden_dim, hidden_dim))\n","        # Theano: Created shared variables\n","        self.U = theano.shared(name='U', value=U.astype(theano.config.floatX))\n","        self.V = theano.shared(name='V', value=V.astype(theano.config.floatX))\n","        self.W = theano.shared(name='W', value=W.astype(theano.config.floatX))      \n","        # We store the Theano graph here\n","        self.theano = {}\n","        self.__theano_build__()\n","    \n","    def __theano_build__(self):\n","        U, V, W = self.U, self.V, self.W\n","        x = T.ivector('x')\n","        y = T.ivector('y')\n","        def forward_prop_step(x_t, s_t_prev, U, V, W):\n","            s_t = T.tanh(U[:,x_t] + W.dot(s_t_prev))\n","            o_t = T.nnet.softmax(V.dot(s_t))\n","            return [o_t[0], s_t]\n","        [o,s], updates = theano.scan(\n","            forward_prop_step,\n","            sequences=x,\n","            outputs_info=[None, dict(initial=T.zeros(self.hidden_dim))],\n","            non_sequences=[U, V, W],\n","            truncate_gradient=self.bptt_truncate,\n","            strict=True)\n","        \n","        prediction = T.argmax(o, axis=1)\n","        o_error = T.sum(T.nnet.categorical_crossentropy(o, y))\n","        \n","        # Gradients\n","        dU = T.grad(o_error, U)\n","        dV = T.grad(o_error, V)\n","        dW = T.grad(o_error, W)\n","        \n","        # Assign functions\n","        self.forward_propagation = theano.function([x], o)\n","        self.predict = theano.function([x], prediction)\n","        self.ce_error = theano.function([x, y], o_error)\n","        self.bptt = theano.function([x, y], [dU, dV, dW])\n","        \n","        # SGD\n","        learning_rate = T.scalar('learning_rate')\n","        self.sgd_step = theano.function([x,y,learning_rate], [], \n","                      updates=[(self.U, self.U - learning_rate * dU),\n","                              (self.V, self.V - learning_rate * dV),\n","                              (self.W, self.W - learning_rate * dW)])\n","    \n","    def calculate_total_loss(self, X, Y):\n","        return np.sum([self.ce_error(x,y) for x,y in zip(X,Y)])\n","    \n","    def calculate_loss(self, X, Y):\n","        # Divide calculate_loss by the number of words\n","        num_words = np.sum([len(y) for y in Y])\n","        return self.calculate_total_loss(X,Y)/float(num_words)   \n","\n","\n","def gradient_check_theano(model, x, y, h=0.001, error_threshold=0.01):\n","    # Overwrite the bptt attribute. We need to backpropagate all the way to get the correct gradient\n","    model.bptt_truncate = 1000\n","    # Calculate the gradients using backprop\n","    bptt_gradients = model.bptt(x, y)\n","    # List of all parameters we want to chec.\n","    model_parameters = ['U', 'V', 'W']\n","    # Gradient check for each parameter\n","    for pidx, pname in enumerate(model_parameters):\n","        # Get the actual parameter value from the mode, e.g. model.W\n","        parameter_T = operator.attrgetter(pname)(model)\n","        parameter = parameter_T.get_value()\n","        print(\"Performing gradient check for parameter %s with size %d.\" % (pname, np.prod(parameter.shape)))\n","        # Iterate over each element of the parameter matrix, e.g. (0,0), (0,1), ...\n","        it = np.nditer(parameter, flags=['multi_index'], op_flags=['readwrite'])\n","        while not it.finished:\n","            ix = it.multi_index\n","            # Save the original value so we can reset it later\n","            original_value = parameter[ix]\n","            # Estimate the gradient using (f(x+h) - f(x-h))/(2*h)\n","            parameter[ix] = original_value + h\n","            parameter_T.set_value(parameter)\n","            gradplus = model.calculate_total_loss([x],[y])\n","            parameter[ix] = original_value - h\n","            parameter_T.set_value(parameter)\n","            gradminus = model.calculate_total_loss([x],[y])\n","            estimated_gradient = (gradplus - gradminus)/(2*h)\n","            parameter[ix] = original_value\n","            parameter_T.set_value(parameter)\n","            # The gradient for this parameter calculated using backpropagation\n","            backprop_gradient = bptt_gradients[pidx][ix]\n","            # calculate The relative error: (|x - y|/(|x| + |y|))\n","            relative_error = np.abs(backprop_gradient - estimated_gradient)/(np.abs(backprop_gradient) + np.abs(estimated_gradient))\n","            # If the error is to large fail the gradient check\n","            if relative_error > error_threshold:\n","                print(\"Gradient Check ERROR: parameter=%s ix=%s\" % (pname, ix))\n","                print(\"+h Loss: %f\" % gradplus)\n","                print(\"-h Loss: %f\" % gradminus)\n","                print(\"Estimated_gradient: %f\" % estimated_gradient)\n","                print(\"Backpropagation gradient: %f\" % backprop_gradient)\n","                print(\"Relative Error: %f\" % relative_error)\n","                return \n","            it.iternext()\n","        print (\"Gradient check for parameter %s passed.\" % (pname))"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dUCIoiKE6SLo","executionInfo":{"status":"ok","timestamp":1617120952111,"user_tz":-420,"elapsed":139147,"user":{"displayName":"Phúc Nguyễn","photoUrl":"","userId":"16708820210452606118"}},"outputId":"b04214e4-e8ab-4803-ac0e-4617af93f97e"},"source":["import csv\n","import itertools\n","import operator\n","import nltk\n","import sys\n","import os\n","import time\n","from datetime import datetime\n","\n","nltk.download(\"book\")\n","\n","_VOCABULARY_SIZE = int(os.environ.get('VOCABULARY_SIZE', '8000'))\n","_HIDDEN_DIM = int(os.environ.get('HIDDEN_DIM', '80'))\n","_LEARNING_RATE = float(os.environ.get('LEARNING_RATE', '0.005'))\n","_NEPOCH = int(os.environ.get('NEPOCH', '100'))\n","_MODEL_FILE = os.environ.get('MODEL_FILE')\n","\n","def train_with_sgd(model, X_train, y_train, learning_rate=0.005, nepoch=1, evaluate_loss_after=5):\n","    losses = []\n","    num_examples_seen = 0\n","    for epoch in range(nepoch):\n","        if (epoch % evaluate_loss_after == 0):\n","            loss = model.calculate_loss(X_train, y_train)\n","            losses.append((num_examples_seen, loss))\n","            time = datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n","            print (\"%s: Loss after num_examples_seen=%d epoch=%d: %f\" % (time, num_examples_seen, epoch, loss))\n","            if (len(losses) > 1 and losses[-1][1] > losses[-2][1]):\n","                learning_rate = learning_rate * 0.5  \n","                print (\"Setting learning rate to %f\" % learning_rate)\n","            sys.stdout.flush()\n","            save_model_parameters_theano(\"./data/rnn-theano-%d-%d-%s.npz\" % (model.hidden_dim, model.word_dim, time), model)\n","        for i in range(len(y_train)):\n","            model.sgd_step(X_train[i], y_train[i], learning_rate)\n","            num_examples_seen += 1\n","\n","#Tạo câu ngẫu nhiên\n","def generate_sentence(model):\n","    new_sentence = [word_to_index[sentence_start_token]]\n","    while not new_sentence[-1] == word_to_index[sentence_end_token]:\n","        next_word_probs = model.forward_propagation(new_sentence)\n","        sampled_word = word_to_index[unknown_token]\n","        while sampled_word == word_to_index[unknown_token]:\n","            samples = np.random.multinomial(1, next_word_probs[-1])\n","            sampled_word = np.argmax(samples)\n","        new_sentence.append(sampled_word)\n","    sentence_str = [index_to_word[x] for x in new_sentence[1:-1]]\n","    return sentence_str\n","\n","#Đoán các từ tiếp theo từ câu cho trước\n","def guess_sentence(sentence, model):\n","  sentence = [\"%s %s\" % (sentence_start_token, sentence)]\n","  tokenized_sentence = nltk.word_tokenize(sentence[0])\n","  tokenized_sentence = [w if w in word_to_index else unknown_token for w in tokenized_sentence]\n","  guess_sent = [word_to_index[w] for w in tokenized_sentence]\n","  while not guess_sent[-1] == word_to_index[sentence_end_token]:\n","    next_word_probs = model.forward_propagation(guess_sent)\n","    sampled_word = word_to_index[unknown_token]\n","    while sampled_word == word_to_index[unknown_token]:\n","      samples = np.random.multinomial(1, next_word_probs[-1])\n","      sampled_word = np.argmax(samples)\n","    guess_sent.append(sampled_word)\n","  sentence_str = [index_to_word[x] for x in guess_sent[1:-1]]\n","  return sentence_str\n","\n","vocabulary_size = _VOCABULARY_SIZE\n","unknown_token = \"UNKNOWN_TOKEN\"\n","sentence_start_token = \"SENTENCE_START\"\n","sentence_end_token = \"SENTENCE_END\"\n","\n","# Đọc dữ liệu và thêm SENTENCE_START và SENTENCE_END \n","print (\"Reading CSV file...\")\n","with open('data/reddit-comments-2015-08.csv', encoding=\"utf8\") as f:\n","    reader = csv.reader(f, skipinitialspace=True)\n","    next(reader, None)\n","    # Chia toàn bộ bình luận thành mảng chứa các câu\n","    sentences = itertools.chain(*[nltk.sent_tokenize(x[0].lower()) for x in reader])\n","    # Thêm SENTENCE_START vào đầu và SENTENCE_END vào cuối\n","    sentences = [\"%s %s %s\" % (sentence_start_token, x, sentence_end_token) for x in sentences]\n","print (\"Parsed %d sentences.\" % (len(sentences)))\n","\n","\n","# Tách từ các câu trong mảng\n","tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n","\n","# Đếm tần số xuất hiện của các từ\n","word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n","print (\"Found %d unique words tokens.\" % len(word_freq.items()))\n","\n","# Tạo mảng từ vựng có các từ thường xuất hiện nhất\n","vocab = word_freq.most_common(vocabulary_size-1)\n","# Tạo vector vị trí đối ứng với từ trong mảng từ vựng\n","index_to_word = [x[0] for x in vocab]\n","index_to_word.append(unknown_token)\n","# Tạo từ điển từ đối ứng với vị trí\n","word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)])\n","\n","print (\"Using vocabulary size %d.\" % vocabulary_size)\n","print (\"The least frequent word in our vocabulary is '%s' and appeared %d times.\" % (vocab[-1][0], vocab[-1][1]))\n","\n","# Thay thế những từ không có trong mảng từ vựng thành 'UNKNOW_TOKEN'\n","for i, sent in enumerate(tokenized_sentences):\n","    tokenized_sentences[i] = [w if w in word_to_index else unknown_token for w in sent]\n","\n","# Tạo dữ liệu traning\n","X_train = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in tokenized_sentences])\n","y_train = np.asarray([[word_to_index[w] for w in sent[1:]] for sent in tokenized_sentences])\n","\n","#Tạo model\n","model = RNNTheano(vocabulary_size, hidden_dim=_HIDDEN_DIM)\n","t1 = time.time()\n","model.sgd_step(X_train[10], y_train[10], _LEARNING_RATE)\n","t2 = time.time()\n","print (\"SGD Step time: %f milliseconds\" % ((t2 - t1) * 1000.))\n","\n","model = RNNTheano(vocabulary_size, hidden_dim=50)\n","# losses = train_with_sgd(model, X_train, y_train, nepoch=50)\n","# save_model_parameters_theano('./data/trained-model-theano.npz', model)\n","load_model_parameters_theano('./data/trained-model-theano.npz', model)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading collection 'book'\n","[nltk_data]    | \n","[nltk_data]    | Downloading package abc to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/abc.zip.\n","[nltk_data]    | Downloading package brown to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/brown.zip.\n","[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/chat80.zip.\n","[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/cmudict.zip.\n","[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/conll2000.zip.\n","[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/conll2002.zip.\n","[nltk_data]    | Downloading package dependency_treebank to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n","[nltk_data]    | Downloading package genesis to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/genesis.zip.\n","[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n","[nltk_data]    | Downloading package ieer to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/ieer.zip.\n","[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/inaugural.zip.\n","[nltk_data]    | Downloading package movie_reviews to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n","[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n","[nltk_data]    | Downloading package names to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/names.zip.\n","[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/ppattach.zip.\n","[nltk_data]    | Downloading package reuters to /root/nltk_data...\n","[nltk_data]    | Downloading package senseval to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/senseval.zip.\n","[nltk_data]    | Downloading package state_union to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/state_union.zip.\n","[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/stopwords.zip.\n","[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/swadesh.zip.\n","[nltk_data]    | Downloading package timit to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/timit.zip.\n","[nltk_data]    | Downloading package treebank to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/treebank.zip.\n","[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/toolbox.zip.\n","[nltk_data]    | Downloading package udhr to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/udhr.zip.\n","[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/udhr2.zip.\n","[nltk_data]    | Downloading package unicode_samples to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n","[nltk_data]    | Downloading package webtext to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/webtext.zip.\n","[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/wordnet.zip.\n","[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n","[nltk_data]    | Downloading package words to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/words.zip.\n","[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n","[nltk_data]    | Downloading package maxent_ne_chunker to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n","[nltk_data]    | Downloading package universal_tagset to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n","[nltk_data]    | Downloading package punkt to /root/nltk_data...\n","[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n","[nltk_data]    | Downloading package book_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n","[nltk_data]    | Downloading package city_database to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/city_database.zip.\n","[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n","[nltk_data]    |   Unzipping help/tagsets.zip.\n","[nltk_data]    | Downloading package panlex_swadesh to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package averaged_perceptron_tagger to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data]    | \n","[nltk_data]  Done downloading collection book\n","Reading CSV file...\n","Parsed 79170 sentences.\n","Found 65499 unique words tokens.\n","Using vocabulary size 8000.\n","The least frequent word in our vocabulary is 'documentary' and appeared 10 times.\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  return array(a, dtype, copy=False, order=order)\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:30: UserWarning: DEPRECATION: If x is a vector, Softmax will not automatically pad x anymore in next releases. If you need it, please do it manually. The vector case is gonna be supported soon and the output will be a vector.\n","WARNING (theano.tensor.blas): We did not find a dynamic library in the library_dir of the library we use for blas. If you use ATLAS, make sure to compile it with dynamics library.\n","WARNING (theano.tensor.blas): We did not find a dynamic library in the library_dir of the library we use for blas. If you use ATLAS, make sure to compile it with dynamics library.\n"],"name":"stderr"},{"output_type":"stream","text":["SGD Step time: 116.662979 milliseconds\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:30: UserWarning: DEPRECATION: If x is a vector, Softmax will not automatically pad x anymore in next releases. If you need it, please do it manually. The vector case is gonna be supported soon and the output will be a vector.\n"],"name":"stderr"},{"output_type":"stream","text":["Loaded model parameters from ./data/trained-model-theano.npz. hidden_dim=50 word_dim=8000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"o0oh8Zbi7DE1","executionInfo":{"status":"ok","timestamp":1617121005360,"user_tz":-420,"elapsed":3848,"user":{"displayName":"Phúc Nguyễn","photoUrl":"","userId":"16708820210452606118"}},"outputId":"e86cfab4-ded0-4a5e-b98f-939e187434a5","colab":{"base_uri":"https://localhost:8080/"}},"source":["num_sentences = 30\n","senten_min_length = 8\n","\n","for i in range(num_sentences):\n","    sent = []\n","    while len(sent) < senten_min_length:\n","        sent = guess_sentence(\"go to hotel \", model)\n","    print(\" \".join(sent))\n","  "],"execution_count":4,"outputs":[{"output_type":"stream","text":["go to hotel of being home won ref .\n","go to hotel of surgery the thousands city is .\n","go to hotel of these helps at all ?\n","go to hotel at hurts or irrelevant .\n","go to hotel letter of the time .\n","go to hotel or pack involved out of people ?\n","go to hotel things to bastard and among .\n","go to hotel of ip dream you .\n","go to hotel of our universe 's charge to be seen .\n","go to hotel it 's just straight .\n","go to hotel and sue one adding .\n","go to hotel works on this sub .\n","go to hotel time ago ; ) .\n","go to hotel you a did idea !\n","go to hotel it could go back in m .\n","go to hotel of new spoilers ) .\n","go to hotel up with this sub .\n","go to hotel of water 's stuff .\n","go to hotel of weapon to say we it .\n","go to hotel of the 3 body *\n","go to hotel the responsible of first races .\n","go to hotel me there is for me .\n","go to hotel for generations more than might etc .\n","go to hotel btw 's sell ) .\n","go to hotel to comparison and +10 .\n","go to hotel you try on other and hi .\n","go to hotel of it playing to the world .\n","go to hotel we that shit is we backpack !\n","go to hotel being habit everything we them .\n","go to hotel or $ 5 receive .\n"],"name":"stdout"}]}]}